"""
LabLeakFinder L5: Post-Exploitation Module
Domain: Post-Exploitation Activities & Impact Validation (PenTest+ Domains 4-5)

Purpose:
    Simulate post-exploitation activities to demonstrate attack impact.
    Establish persistence mechanisms to maintain access.
    Simulate lateral movement and privilege escalation.
    Demonstrate data exfiltration capabilities.
    Validate business consequences of successful compromise.

Key Functions:
    - Persistence simulation (scheduled tasks, backdoors, services)
    - Lateral movement & pivoting (SMB, WMI, RPC simulation)
    - Privilege escalation attempts
    - Data discovery & classification
    - Exfiltration simulation (sensitive data identification)
    - Impact quantification (financial, operational, compliance)
    - Remediation validation (before/after metrics)
    - Attack timeline & forensics evidence

Integration:
    INPUT: L4 Exploitation Report (exploitation_report.json)
    INPUT: L3 Findings (findings_report.json)
    OUTPUT:
        - post_exploitation_report.json (persistence + lateral movement)
        - data_discovery_log.json (sensitive data identified)
        - business_impact_assessment.json (financial consequences)
        - remediation_impact_validation.html (control effectiveness)
        - final_penetration_test_report.html (executive deliverable)

PenTest+ Alignment:
    - Domain 2: Information Gathering (L1/L2/L3)
    - Domain 3: Vulnerability Identification (L3)
    - Domain 4: Penetration Testing (L4)
    - Domain 4: Post-Exploitation (L5) ← YOU ARE HERE
    - Domain 4: Social Engineering (included in persistence scenarios)
    - Domain 5: Data Assessment & Reporting (L5)
    - Domain 6: Reporting & Communication (final report)

Ethical Boundaries:
    - Lab environment only - no real systems compromised
    - Simulation mode - no actual persistence installed
    - No real data exfiltrated
    - All activities logged for audit and remediation validation
"""

import json
import logging
import sys
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict
import random

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('labfinder_l5_detailed.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class PersistenceMethod(Enum):
    """Methods to establish persistence on compromised systems."""
    SCHEDULED_TASK = "scheduled_task"
    CRON_JOB = "cron_job"
    BACKDOOR_USER = "backdoor_user"
    REVERSE_SHELL = "reverse_shell"
    WEBSHELL = "webshell"
    SERVICE = "service"
    REGISTRY = "registry"

class LateralMovementTechnique(Enum):
    """Techniques for lateral movement to other systems."""
    SMB = "smb"
    WMI = "wmi"
    PSEXEC = "psexec"
    WINRM = "winrm"
    SSH = "ssh"
    RDP = "rdp"

class DataClassification(Enum):
    """Classification level of discovered data."""
    CONFIDENTIAL = "confidential"
    RESTRICTED = "restricted"
    INTERNAL = "internal"
    PUBLIC = "public"

@dataclass
class PersistenceMechanism:
    """Record of a persistence mechanism established."""
    mechanism_id: str
    method: PersistenceMethod
    host: str
    location: str
    trigger: str
    detection_difficulty: str
    removal_steps: List[str]
    timestamp: str

@dataclass
class LateralMovementAttempt:
    """Record of lateral movement attempt."""
    movement_id: str
    source_host: str
    target_host: str
    technique: LateralMovementTechnique
    credentials_used: str
    success: bool
    access_level: str
    systems_compromised: List[str]
    timestamp: str

@dataclass
class SensitiveData:
    """Sensitive data discovered during post-exploitation."""
    data_id: str
    data_type: str
    location: str
    classification: DataClassification
    record_count: int
    sample_records: List[str]
    risk_level: str
    timestamp: str

class PostExploitationAnalyzer:
    """
    Simulates post-exploitation activities and impact assessment.
    
    Implements:
    - Persistence establishment simulation
    - Lateral movement and pivoting
    - Privilege escalation
    - Data discovery and classification
    - Exfiltration impact quantification
    - Business consequence modeling
    """
    
    def __init__(self, exploitation_file: str = "exploitation_report.json"):
        self.exploitations = self._load_exploitations(exploitation_file)
        self.persistence_mechanisms = []
        self.lateral_movements = []
        self.discovered_data = []
        self.mechanism_counter = 0
        self.movement_counter = 0
        self.data_counter = 0
        
        logger.info(f"PostExploitationAnalyzer initialized with {len(self.exploitations)} exploitations")
    
    def _load_exploitations(self, filename: str) -> List[Dict]:
        """Load exploitation data from L4."""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('exploitation_attempts', [])
        except FileNotFoundError:
            logger.error(f"Exploitation file not found: {filename}")
            return []
    
    def establish_persistence(self) -> List[PersistenceMechanism]:
        """Simulate establishing persistence mechanisms on compromised hosts."""
        logger.info("Establishing persistence mechanisms on compromised hosts")
        
        persistence_scenarios = {
            "vulnerable.lab": [
                {
                    "method": PersistenceMethod.SCHEDULED_TASK,
                    "location": "C:\\Windows\\System32\\Tasks\\SystemUpdate",
                    "trigger": "Every 6 hours at system startup",
                    "payload": "Reverse shell to attacker C&C server (192.168.100.50:4444)"
                },
                {
                    "method": PersistenceMethod.BACKDOOR_USER,
                    "location": "Local user account: 'BackupAdmin'",
                    "trigger": "User login (password: Backdoor@2024!)",
                    "payload": "Remote Desktop enabled, admin privileges granted"
                },
                {
                    "method": PersistenceMethod.WEBSHELL,
                    "location": "/var/www/html/admin/.cache/update.php",
                    "trigger": "HTTP request: GET /admin/.cache/update.php?cmd=...",
                    "payload": "Web-based shell for command execution"
                }
            ]
        }
        
        for target_domain, scenarios in persistence_scenarios.items():
            for scenario in scenarios:
                self.mechanism_counter += 1
                mechanism = PersistenceMechanism(
                    mechanism_id=f"PERS-{self.mechanism_counter:04d}",
                    method=scenario['method'],
                    host=target_domain,
                    location=scenario['location'],
                    trigger=scenario['trigger'],
                    detection_difficulty="Medium" if scenario['method'] in [PersistenceMethod.SCHEDULED_TASK, PersistenceMethod.CRON_JOB] else "High",
                    removal_steps=self._generate_removal_steps(scenario['method']),
                    timestamp=datetime.now().isoformat()
                )
                self.persistence_mechanisms.append(mechanism)
                logger.info(f"[PERSIST] {mechanism.mechanism_id}: {scenario['method'].value} on {target_domain}")
        
        logger.info(f"Persistence phase complete. {len(self.persistence_mechanisms)} mechanisms established.")
        return self.persistence_mechanisms
    
    @staticmethod
    def _generate_removal_steps(method: PersistenceMethod) -> List[str]:
        """Generate remediation steps to remove persistence mechanism."""
        removal_map = {
            PersistenceMethod.SCHEDULED_TASK: [
                "Open Task Scheduler",
                "Navigate to: \\Microsoft\\Windows\\",
                "Locate and delete malicious task",
                "Verify removal with: tasklist /FI \"IMAGENAME eq task_name.exe\""
            ],
            PersistenceMethod.BACKDOOR_USER: [
                "Open Computer Management > Local Users and Groups",
                "Right-click backdoor account > Delete",
                "Remove from admin group if not deleted",
                "Verify with: net user"
            ],
            PersistenceMethod.WEBSHELL: [
                "Connect to web server via SFTP/SSH",
                "Locate shell file path",
                "Delete malicious .php file",
                "Verify with: find /var/www -name '*.php' -newer <date>"
            ],
            PersistenceMethod.REVERSE_SHELL: [
                "Kill process: pkill -f 'nc -l'",
                "Remove cron entry: crontab -e (delete malicious line)",
                "Check netstat for open ports: netstat -tlnp",
                "Verify: ss -tlnp | grep LISTEN"
            ]
        }
        return removal_map.get(method, ["Manual inspection required"])
    
    def perform_lateral_movement(self) -> List[LateralMovementAttempt]:
        """Simulate lateral movement to other systems in network."""
        logger.info("Performing lateral movement and pivoting")
        
        # Simulated internal network structure
        internal_network = {
            "vulnerable.lab": {
                "systems": ["database_server.lab", "app_server2.lab", "admin_workstation.lab"],
                "credentials_available": "root/MyS3cr3tP@ss (from backup file)",
                "techniques": [
                    LateralMovementTechnique.SMB,
                    LateralMovementTechnique.WMI,
                    LateralMovementTechnique.SSH
                ]
            }
        }
        
        for source, network_info in internal_network.items():
            for target in network_info['systems']:
                for technique in network_info['techniques']:
                    self.movement_counter += 1
                    
                    # Simulate success based on technique and credentials
                    success = technique in [LateralMovementTechnique.SMB, LateralMovementTechnique.SSH]
                    
                    attempt = LateralMovementAttempt(
                        movement_id=f"MOVE-{self.movement_counter:04d}",
                        source_host=source,
                        target_host=target,
                        technique=technique,
                        credentials_used=network_info['credentials_available'],
                        success=success,
                        access_level="Administrator" if success else "None",
                        systems_compromised=[target] if success else [],
                        timestamp=datetime.now().isoformat()
                    )
                    self.lateral_movements.append(attempt)
                    logger.info(f"[LATERAL] {attempt.movement_id}: {technique.value} to {target} - {'Success' if success else 'Failed'}")
        
        logger.info(f"Lateral movement phase complete. {sum(1 for m in self.lateral_movements if m.success)} successful movements.")
        return self.lateral_movements
    
    def discover_sensitive_data(self) -> List[SensitiveData]:
        """Simulate discovery and classification of sensitive data."""
        logger.info("Discovering and classifying sensitive data")
        
        data_discovery_scenarios = [
            {
                "type": "User Credentials Database",
                "location": "vulnerable.lab:/var/lib/mysql/users_table.sql",
                "records": 156250,
                "samples": ["user@example.com:$2y$10$...", "admin@company.com:$2y$10$..."],
                "classification": DataClassification.CONFIDENTIAL
            },
            {
                "type": "Payment Card Data (PCI-DSS)",
                "location": "database_server.lab:/data/payments/transactions.db",
                "records": 45892,
                "samples": ["4532-1234-5678-9012", "5425-2345-6789-0123"],
                "classification": DataClassification.CONFIDENTIAL
            },
            {
                "type": "API Keys and Secrets",
                "location": "app_server2.lab:/etc/.env, /var/www/config/",
                "records": 127,
                "samples": ["sk-proj-abc123def456...", "STRIPE_SECRET_KEY=..."],
                "classification": DataClassification.CONFIDENTIAL
            },
            {
                "type": "Employee Personal Data",
                "location": "admin_workstation.lab:/Users/SharedDrive/HR/",
                "records": 892,
                "samples": ["SSN:123-45-6789", "DOB:1990-01-15"],
                "classification": DataClassification.RESTRICTED
            },
            {
                "type": "Source Code Repository",
                "location": "database_server.lab:/git/application/.git/",
                "records": 2847,
                "samples": ["admin password: hardcoded_secret", "API_KEY in code"],
                "classification": DataClassification.RESTRICTED
            },
            {
                "type": "Customer Contact Information",
                "location": "vulnerable.lab:/var/www/html/export/customers_2024.csv",
                "records": 287654,
                "samples": ["John Doe, john@example.com, 555-1234", "Jane Smith, jane@company.com, 555-5678"],
                "classification": DataClassification.INTERNAL
            }
        ]
        
        for scenario in data_discovery_scenarios:
            self.data_counter += 1
            data = SensitiveData(
                data_id=f"DATA-{self.data_counter:04d}",
                data_type=scenario['type'],
                location=scenario['location'],
                classification=scenario['classification'],
                record_count=scenario['records'],
                sample_records=scenario['samples'],
                risk_level="Critical" if scenario['classification'] == DataClassification.CONFIDENTIAL else "High",
                timestamp=datetime.now().isoformat()
            )
            self.discovered_data.append(data)
            logger.info(f"[DATA] {data.data_id}: {scenario['type']} - {scenario['records']} records")
        
        logger.info(f"Data discovery complete. {len(self.discovered_data)} sensitive data sources identified.")
        return self.discovered_data
    
    def calculate_business_impact(self) -> Dict:
        """Calculate financial and operational impact of compromise."""
        logger.info("Calculating business impact of successful compromise")
        
        # NIST Cybersecurity Framework impact metrics
        total_records_exposed = sum(d.record_count for d in self.discovered_data)
        critical_systems_compromised = len(set(m.target_host for m in self.lateral_movements if m.success))
        
        # Financial impact calculation (based on industry standards)
        cost_per_record = 150  # NIST average: $150 per exposed record
        data_breach_cost = total_records_exposed * cost_per_record
        
        # Operational downtime cost ($5000-$15000 per hour for enterprise)
        estimated_downtime_hours = 48
        downtime_cost = estimated_downtime_hours * 10000
        
        # Regulatory fines (GDPR: up to 4% of global revenue, CCPA: $100-$750 per record)
        regulatory_fine_conservative = min(total_records_exposed * 100, 5000000)
        
        # Reputational damage (average 15-25% customer churn in data breaches)
        estimated_annual_revenue = 50000000
        customer_churn_rate = 0.20
        churn_cost = estimated_annual_revenue * customer_churn_rate
        
        total_financial_impact = data_breach_cost + downtime_cost + regulatory_fine_conservative + churn_cost
        
        return {
            "total_records_exposed": total_records_exposed,
            "critical_systems_compromised": critical_systems_compromised,
            "persistence_mechanisms": len(self.persistence_mechanisms),
            "successful_lateral_movements": sum(1 for m in self.lateral_movements if m.success),
            "data_breach_notification_cost": data_breach_cost,
            "operational_downtime_cost": downtime_cost,
            "regulatory_fines": regulatory_fine_conservative,
            "reputational_damage_cost": churn_cost,
            "total_financial_impact": total_financial_impact,
            "time_to_detect": "3-6 months (industry average)",
            "recovery_timeline": "6-12 months",
            "business_continuity_impact": "Complete disruption of core services"
        }
    
    def export_post_exploitation_report(self, filename: str = "post_exploitation_report.json") -> None:
        """Export post-exploitation activities and findings."""
        report = {
            "report_generated": datetime.now().isoformat(),
            "phase": "Post-Exploitation Activities",
            "persistence_mechanisms": [
                {
                    "mechanism_id": m.mechanism_id,
                    "method": m.method.value,
                    "host": m.host,
                    "location": m.location,
                    "trigger": m.trigger,
                    "detection_difficulty": m.detection_difficulty,
                    "removal_steps": m.removal_steps
                }
                for m in self.persistence_mechanisms
            ],
            "lateral_movements": [
                {
                    "movement_id": m.movement_id,
                    "source": m.source_host,
                    "target": m.target_host,
                    "technique": m.technique.value,
                    "credentials": m.credentials_used,
                    "success": m.success,
                    "access_level": m.access_level,
                    "systems_compromised": m.systems_compromised
                }
                for m in self.lateral_movements
            ],
            "data_discovered": [
                {
                    "data_id": d.data_id,
                    "type": d.data_type,
                    "location": d.location,
                    "classification": d.classification.value,
                    "record_count": d.record_count,
                    "risk_level": d.risk_level,
                    "sample_data": d.sample_records
                }
                for d in self.discovered_data
            ],
            "business_impact": self.calculate_business_impact()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Exported post-exploitation report to {filename}")
    
    def export_data_discovery_log(self, filename: str = "data_discovery_log.json") -> None:
        """Export detailed data discovery findings."""
        discovery_log = {
            "report_generated": datetime.now().isoformat(),
            "phase": "Data Discovery & Classification",
            "total_data_sources": len(self.discovered_data),
            "total_records_at_risk": sum(d.record_count for d in self.discovered_data),
            "data_sources": [
                {
                    "data_id": d.data_id,
                    "type": d.data_type,
                    "location": d.location,
                    "classification": d.classification.value,
                    "record_count": d.record_count,
                    "sample_records": d.sample_records,
                    "risk_level": d.risk_level,
                    "exfiltration_time": f"{d.record_count / 1000 / 10:.1f} minutes over 10 Mbps connection",
                    "compliance_impact": f"Breach notification required - {d.record_count} records",
                    "remediation": f"Encrypt at rest, implement access controls, audit logs"
                }
                for d in sorted(self.discovered_data, key=lambda x: x.record_count, reverse=True)
            ]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(discovery_log, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Exported data discovery log to {filename}")
    
    def export_business_impact_report(self, filename: str = "business_impact_assessment.json") -> None:
        """Export business impact assessment."""
        impact = self.calculate_business_impact()
        
        report = {
            "report_generated": datetime.now().isoformat(),
            "executive_summary": {
                "overall_risk_rating": "CRITICAL",
                "likelihood_of_occurrence": "High (multiple exploitable vulnerabilities present)",
                "impact_if_exploited": "Catastrophic - Full infrastructure compromise",
                "estimated_financial_loss": f"${impact['total_financial_impact']:,.0f}"
            },
            "financial_impact": {
                "data_breach_notification": impact['data_breach_notification_cost'],
                "operational_downtime": impact['operational_downtime_cost'],
                "regulatory_fines": impact['regulatory_fines'],
                "reputational_damage": impact['reputational_damage_cost'],
                "total_loss": impact['total_financial_impact']
            },
            "operational_impact": {
                "systems_compromised": impact['critical_systems_compromised'],
                "records_exposed": impact['total_records_exposed'],
                "time_to_detect": impact['time_to_detect'],
                "recovery_timeline": impact['recovery_timeline'],
                "business_continuity": impact['business_continuity_impact']
            },
            "compliance_impact": {
                "regulations_violated": ["GDPR", "CCPA", "HIPAA", "PCI-DSS"],
                "notification_requirements": f"Notify {impact['total_records_exposed']:,} individuals",
                "regulatory_exposure": f"${impact['regulatory_fines']:,}",
                "board_notification": "Required - within 4 weeks of discovery"
            },
            "recommendations": [
                "IMMEDIATE: Implement network segmentation to prevent lateral movement",
                "IMMEDIATE: Disable exposed admin accounts and rotate all credentials",
                "URGENT: Deploy endpoint detection and response (EDR) solution",
                "URGENT: Implement WAF to block exploitation attempts",
                "Priority 1: Patch all identified vulnerabilities within 48 hours",
                "Priority 1: Deploy DLP solution to prevent data exfiltration",
                "Priority 2: Implement MFA on all administrative accounts",
                "Priority 2: Deploy SIEM for centralized monitoring"
            ]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Exported business impact assessment to {filename}")

def main():
    """Demonstrate L5 Post-Exploitation Module functionality."""
    
    logger.info("=" * 80)
    logger.info("LabLeakFinder - L5 Post-Exploitation Module")
    logger.info("Domain: Post-Exploitation Activities & Impact Validation")
    logger.info("=" * 80)
    
    analyzer = PostExploitationAnalyzer(exploitation_file="exploitation_report.json")
    
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 1: PERSISTENCE ESTABLISHMENT")
    logger.info("=" * 80)
    
    persistence = analyzer.establish_persistence()
    logger.info(f"\nEstablished {len(persistence)} persistence mechanisms:")
    for p in persistence:
        logger.info(f"  {p.mechanism_id}: {p.method.value} on {p.host}")
    
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 2: LATERAL MOVEMENT & PIVOTING")
    logger.info("=" * 80)
    
    movements = analyzer.perform_lateral_movement()
    successful = [m for m in movements if m.success]
    logger.info(f"\nLateral Movement Attempts: {len(movements)}")
    logger.info(f"Successful Movements: {len(successful)}")
    for m in successful:
        logger.info(f"  {m.movement_id}: {m.source_host} → {m.target_host} ({m.technique.value})")
    
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 3: DATA DISCOVERY & CLASSIFICATION")
    logger.info("=" * 80)
    
    data = analyzer.discover_sensitive_data()
    total_records = sum(d.record_count for d in data)
    logger.info(f"\nData Sources Discovered: {len(data)}")
    logger.info(f"Total Records at Risk: {total_records:,}")
    
    for d in sorted(data, key=lambda x: x.record_count, reverse=True)[:5]:
        logger.info(f"  {d.data_id}: {d.data_type} - {d.record_count:,} records ({d.classification.value})")
    
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 4: BUSINESS IMPACT CALCULATION")
    logger.info("=" * 80)
    
    impact = analyzer.calculate_business_impact()
    logger.info(f"\nFinancial Impact Assessment:")
    logger.info(f"  Data Breach Notification: ${impact['data_breach_notification_cost']:,.0f}")
    logger.info(f"  Operational Downtime: ${impact['operational_downtime_cost']:,.0f}")
    logger.info(f"  Regulatory Fines: ${impact['regulatory_fines']:,.0f}")
    logger.info(f"  Reputational Damage: ${impact['reputational_damage_cost']:,.0f}")
    logger.info(f"  ─────────────────────────────────")
    logger.info(f"  TOTAL FINANCIAL IMPACT: ${impact['total_financial_impact']:,.0f}")
    
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 5: REPORT GENERATION")
    logger.info("=" * 80)
    
    analyzer.export_post_exploitation_report("post_exploitation_report.json")
    analyzer.export_data_discovery_log("data_discovery_log.json")
    analyzer.export_business_impact_report("business_impact_assessment.json")
    
    logger.info("\n✓ post_exploitation_report.json - Persistence + lateral movement details")
    logger.info("✓ data_discovery_log.json - Sensitive data classification")
    logger.info("✓ business_impact_assessment.json - Financial impact quantification")
    
    logger.info("\n" + "=" * 80)
    logger.info("L5 Post-Exploitation Module Complete")
    logger.info("Ready for Final Penetration Test Report Generation (Week 6)")
    logger.info("=" * 80)

if __name__ == "__main__":
    main()
